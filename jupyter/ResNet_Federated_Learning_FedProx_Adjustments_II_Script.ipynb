{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iWLbGlg1LLT7",
   "metadata": {
    "id": "iWLbGlg1LLT7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping flwr as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting flwr==1.23.0\n",
      "  Downloading flwr-1.23.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.0/732.0 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio-health-checking<2.0.0,>=1.62.3\n",
      "  Downloading grpcio_health_checking-1.76.0-py3-none-any.whl (18 kB)\n",
      "Collecting tomli<3.0.0,>=2.0.1\n",
      "  Downloading tomli-2.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting typer<0.13.0,>=0.12.5\n",
      "  Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click<8.2.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml<7.0.0,>=6.0.2\n",
      "  Downloading pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.3/770.3 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio!=1.65.0,<2.0.0,>=1.62.3\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathspec<0.13.0,>=0.12.1\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Collecting requests<3.0.0,>=2.31.0\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography<45.0.0,>=44.0.1\n",
      "  Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tomli-w<2.0.0,>=1.0.0\n",
      "  Downloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.6\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<3.0.0,>=1.26.0\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodome<4.0.0,>=3.18.0\n",
      "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich<14.0.0,>=13.5.0\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting iterators<0.0.3,>=0.0.2\n",
      "  Downloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=4.12 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from grpcio!=1.65.0,<2.0.0,>=1.62.3->flwr==1.23.0) (4.15.0)\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.62.3\n",
      "  Downloading grpcio_health_checking-1.75.1-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.75.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.74.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.73.1-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.73.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.72.2-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.72.1-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.71.2-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.71.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.70.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.69.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.68.1-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.68.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.67.1-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.67.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.66.2-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.66.1-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.66.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.65.5-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.65.4-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.65.2-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.65.1-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.64.3-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.64.1-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.64.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.63.2-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.63.0-py3-none-any.whl (18 kB)\n",
      "  Downloading grpcio_health_checking-1.62.3-py3-none-any.whl (18 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.6.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/153.6 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from rich<14.0.0,>=13.5.0->flwr==1.23.0) (2.19.2)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: urllib3, tomli-w, tomli, shellingham, pyyaml, pycryptodome, pycparser, protobuf, pathspec, numpy, mdurl, iterators, idna, grpcio, click, charset_normalizer, certifi, requests, markdown-it-py, grpcio-health-checking, cffi, rich, cryptography, typer, flwr\n",
      "Successfully installed certifi-2025.11.12 cffi-2.0.0 charset_normalizer-3.4.4 click-8.1.8 cryptography-44.0.3 flwr-1.23.0 grpcio-1.76.0 grpcio-health-checking-1.62.3 idna-3.11 iterators-0.0.2 markdown-it-py-4.0.0 mdurl-0.1.2 numpy-2.2.6 pathspec-0.12.1 protobuf-4.25.8 pycparser-2.23 pycryptodome-3.23.0 pyyaml-6.0.3 requests-2.32.5 rich-13.9.4 shellingham-1.5.4 tomli-2.3.0 tomli-w-1.2.0 typer-0.12.5 urllib3-2.6.0\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m389.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.3.90\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.3.83\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.8.93\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.8.93\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx>=2.5.1\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.27.5\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.7.1\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvshmem-cu12==3.3.20\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m883.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=0.8.5\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Collecting triton==3.5.1\n",
      "  Downloading triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.3/170.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "^C\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.1-cp310-cp310-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Collecting torch==2.9.1\n",
      "^C\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=3\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.0 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-12.0.0 pyparsing-3.2.5\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (4.25.8)\n",
      "Requirement already satisfied: six>=1.10 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (1.17.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-slugify\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (59.6.0)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (2.6.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (2025.11.12)\n",
      "Requirement already satisfied: charset-normalizer in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (3.4.4)\n",
      "Collecting text-unidecode\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bleach\n",
      "  Downloading bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (3.11)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /mnt/c/Users/marcella_st_ana/Documents/pos_graduacao/materias/topicos_aprendizado_federado/trabalho_pratico/codes/FederatedLearning/venv/lib/python3.10/site-packages (from kaggle) (2.32.5)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, text-unidecode, tqdm, python-slugify, bleach, kaggle\n",
      "Successfully installed bleach-6.3.0 kaggle-1.7.4.5 python-slugify-8.0.4 text-unidecode-1.3 tqdm-4.67.1 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y flwr\n",
    "!pip install flwr==1.23.0\n",
    "\n",
    "# !pip install -q flwr[simulation]\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install matplotlib\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02824e8f",
   "metadata": {
    "id": "02824e8f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"FLWR_SIMULATION_USE_RAY\"] = \"0\"\n",
    "os.environ[\"FLWR_LOGGING\"] = \"error\"\n",
    "\n",
    "import random\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import flwr as fl\n",
    "# from flwr.common import Context\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# (ADJUSTMENTS) NOVAS BIBLIOTECAS:\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ff072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41212d56",
   "metadata": {
    "id": "41212d56"
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10678db6",
   "metadata": {
    "id": "10678db6"
   },
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 7\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "ROUNDS = 22\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 224 # Imagenet size (224x224)\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "DATA_TRANSFORM = {\n",
    "    \"img_size\": IMG_SIZE,\n",
    "    \"normalizer\": { # Normalizer for Imagenet\n",
    "        \"mean\":[0.485, 0.456, 0.406],\n",
    "        \"std\":[0.229, 0.224, 0.225]\n",
    "    }\n",
    "}\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    'criterion': nn.CrossEntropyLoss(),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'patience': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eniCMPCMLNdh",
   "metadata": {
    "id": "eniCMPCMLNdh"
   },
   "outputs": [],
   "source": [
    "# (ADJUSTMENTS) REMOVENDO AS CONFIGURAÇÕES DO GOOGLE DRIVE, PARA EXECUÇÃO LOCAL.\n",
    "'''\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    os.chdir(\"/content/drive/MyDrive/MO839-FL\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130f831",
   "metadata": {
    "id": "0130f831",
    "outputId": "69785ddf-a67e-42a2-f27b-5a11c412b824"
   },
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60d566",
   "metadata": {
    "id": "7b60d566"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1399142",
   "metadata": {
    "id": "f1399142",
    "outputId": "4ff88ebf-482a-4f0e-9046-e53f50a2d6c1"
   },
   "outputs": [],
   "source": [
    "kidney_dataset = pd.read_csv('G:\\\\Meu Drive\\\\Materias\\\\MO839-FL\\\\trabalho_pratico\\\\simulações_scripts\\\\data\\\\kidneyData.csv')\n",
    "kidney_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6115efa",
   "metadata": {
    "id": "d6115efa"
   },
   "outputs": [],
   "source": [
    "LABELS = sorted(kidney_dataset['Class'].unique())\n",
    "LABEL2TARGET = {str(l): i for i, l in enumerate(LABELS)}\n",
    "NUM_CLASSES = len(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f1343",
   "metadata": {
    "id": "426f1343",
    "outputId": "3a1f5ffe-9ba7-43a5-bc2f-2d5a99fc4654"
   },
   "outputs": [],
   "source": [
    "LABEL2TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss:\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(kidney_dataset[\"Class\"]),\n",
    "    y=kidney_dataset[\"Class\"]\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "\n",
    "MODEL_CONFIG['criterion'] = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffb090",
   "metadata": {
    "id": "7dffb090"
   },
   "outputs": [],
   "source": [
    "class KidneyData(Dataset):\n",
    "    def __init__(self, metadata: pd.DataFrame, label_map: dict, transform=None):\n",
    "        self.metadata = metadata.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        img_path = row['path']\n",
    "        label_str = str(row['Class'])\n",
    "        label = int(self.label_map[label_str])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc8d8d",
   "metadata": {
    "id": "b0cc8d8d",
    "outputId": "157cb5a5-ff73-4721-8c6d-11a336ce977c"
   },
   "outputs": [],
   "source": [
    "classes = kidney_dataset['Class'].unique()\n",
    "\n",
    "fig, ax = plt.subplots(1, len(classes), figsize=(12, 12))\n",
    "for i, c in enumerate(classes):\n",
    "    class_df = kidney_dataset[kidney_dataset['Class'] == c]\n",
    "    random_integer = random.randint(1, 10)\n",
    "    data = KidneyData(class_df, label_map=LABEL2TARGET)\n",
    "    img, label = data.__getitem__(random.randint(1, len(class_df)))\n",
    "    print(f\"image {c} format {np.array(img).shape}\")\n",
    "    img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)\n",
    "    plt.subplot(1, len(classes), i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(c)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b79db9",
   "metadata": {
    "id": "89b79db9"
   },
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba52a24",
   "metadata": {
    "id": "1ba52a24"
   },
   "source": [
    "### Separating some data for testing global model and get clients data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe6bfb",
   "metadata": {
    "id": "9dbe6bfb",
    "outputId": "37d95f5c-286e-4f62-9bd9-b030a4b41ded"
   },
   "outputs": [],
   "source": [
    "# Preserving classes distribution on test set\n",
    "splitter = StratifiedGroupKFold(n_splits=int(1/TEST_SIZE), shuffle=True, random_state=RANDOM_SEED)\n",
    "clients_idx, test_idx = next(splitter.split(\n",
    "    kidney_dataset,\n",
    "    kidney_dataset[\"Class\"],\n",
    "    groups=kidney_dataset[\"group\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "clients_df = kidney_dataset.iloc[clients_idx].reset_index(drop=True)\n",
    "test_df  = kidney_dataset.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"Clients groups:\", clients_df[\"group\"].nunique())\n",
    "print(\"Test groups:\", test_df[\"group\"].nunique())\n",
    "print(\"Intersection:\", set(clients_df[\"group\"]) & set(test_df[\"group\"]))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "sns.countplot(x=test_df['Class'])\n",
    "plt.title(\"Data available to server only for test model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f307b9",
   "metadata": {
    "id": "94f307b9"
   },
   "source": [
    "### Get clients (not available data for server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40900b",
   "metadata": {
    "id": "2a40900b"
   },
   "outputs": [],
   "source": [
    "def split_on_clients(clients_df: pd.DataFrame, num_clients: int = NUM_CLIENTS, val_frac: float = VALID_SIZE):\n",
    "    \"\"\"\n",
    "    Splits data in NUM_CLIENTS clients, randomly, splitting in train and validation data inside each client (like small datasets)\n",
    "\n",
    "    \"\"\"\n",
    "    unique_groups = clients_df[\"group\"].unique()\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(unique_groups)\n",
    "    group_splits = np.array_split(unique_groups, num_clients)\n",
    "\n",
    "    clients = {}\n",
    "\n",
    "    for i, split in enumerate(group_splits):\n",
    "        client_df = clients_df[clients_df[\"group\"].isin(split)]\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size = val_frac, random_state=RANDOM_SEED)\n",
    "        train_idx, val_idx = next(splitter.split(client_df, client_df[\"Class\"], groups=client_df[\"group\"]))\n",
    "\n",
    "        client_train_df = client_df.iloc[train_idx].reset_index(drop=True)\n",
    "        client_val_df = client_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        clients[i] = {'train': client_train_df, 'valid': client_val_df}\n",
    "\n",
    "    return clients\n",
    "\n",
    "# def split_on_clients(clients_df, num_clients=NUM_CLIENTS, val_frac=VALID_SIZE):\n",
    "#     groups = list(clients_df[\"group\"].unique())\n",
    "#     groups.sort()\n",
    "\n",
    "#     # calcula tamanho de cada grupo\n",
    "#     group_sizes = {g: len(clients_df[clients_df[\"group\"] == g]) for g in groups}\n",
    "\n",
    "#     # ordena grupos do maior para menor\n",
    "#     groups_sorted = sorted(groups, key=lambda g: group_sizes[g], reverse=True)\n",
    "\n",
    "#     # distribui grupos balanceando o total de imagens por cliente\n",
    "#     client_buckets = [[] for _ in range(num_clients)]\n",
    "#     bucket_sizes = [0] * num_clients\n",
    "\n",
    "#     for g in groups_sorted:\n",
    "#         smallest = bucket_sizes.index(min(bucket_sizes))\n",
    "#         client_buckets[smallest].append(g)\n",
    "#         bucket_sizes[smallest] += group_sizes[g]\n",
    "\n",
    "#     clients = {}\n",
    "#     for cid in range(num_clients):\n",
    "#         cid_df = clients_df[clients_df[\"group\"].isin(client_buckets[cid])]\n",
    "\n",
    "#         splitter = GroupShuffleSplit(n_splits=1, test_size=val_frac, random_state=RANDOM_SEED)\n",
    "#         train_idx, val_idx = next(splitter.split(cid_df, cid_df[\"Class\"], groups=cid_df[\"group\"]))\n",
    "\n",
    "#         clients[cid] = {\n",
    "#             \"train\": cid_df.iloc[train_idx].reset_index(drop=True),\n",
    "#             \"valid\": cid_df.iloc[val_idx].reset_index(drop=True)\n",
    "#         }\n",
    "\n",
    "#     return clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b873a3",
   "metadata": {
    "id": "c8b873a3"
   },
   "outputs": [],
   "source": [
    "# (ADJUSTMENTS)\n",
    "def split_on_non_iid_clients(clients_df: pd.DataFrame,\n",
    "                             num_clients: int = NUM_CLIENTS,\n",
    "                             val_frac: float = VALID_SIZE,\n",
    "                             alpha: float = 0.5):\n",
    "    \"\"\"\n",
    "    Non-IID usando distribuição Dirichlet.\n",
    "    - Garante todas as classes em todos os clientes\n",
    "    - Mantém heterogeneidade controlada\n",
    "    - Evita clientes com classes faltando\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    clients = {}\n",
    "\n",
    "    # Converte labels para numpy\n",
    "    labels = clients_df[\"Class\"].values\n",
    "    unique_classes = np.unique(labels)\n",
    "\n",
    "    # Índices do dataframe completo\n",
    "    all_indices = np.arange(len(clients_df))\n",
    "\n",
    "    # Lista de índices para cada cliente\n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "\n",
    "    # Para cada classe, divide seus índices entre os clientes\n",
    "    for cls in unique_classes:\n",
    "        cls_idx = all_indices[labels == cls]\n",
    "\n",
    "        # distribuição Dirichlet (controla Non-IID)\n",
    "        proportions = np.random.dirichlet(alpha=[alpha] * num_clients)\n",
    "        proportions = (proportions * len(cls_idx)).astype(int)\n",
    "\n",
    "        start = 0\n",
    "        for cid, amount in enumerate(proportions):\n",
    "            client_indices[cid].extend(cls_idx[start:start + amount])\n",
    "            start += amount\n",
    "\n",
    "    # Embaralha\n",
    "    for cid in range(num_clients):\n",
    "        np.random.shuffle(client_indices[cid])\n",
    "\n",
    "        cid_df = clients_df.iloc[client_indices[cid]].reset_index(drop=True)\n",
    "\n",
    "        # split train/validation por cliente\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=val_frac, random_state=42)\n",
    "        train_idx, val_idx = next(splitter.split(cid_df, cid_df[\"Class\"], groups=cid_df[\"group\"]))\n",
    "\n",
    "        client_train_df = cid_df.iloc[train_idx].reset_index(drop=True)\n",
    "        client_val_df = cid_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        clients[cid] = {'train': client_train_df, 'valid': client_val_df}\n",
    "\n",
    "    return clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030273b",
   "metadata": {
    "id": "2030273b",
    "outputId": "40c539af-a0ec-492b-ba83-4094313e6dd7"
   },
   "outputs": [],
   "source": [
    "random_clients = split_on_clients(clients_df, NUM_CLIENTS, VALID_SIZE)\n",
    "\n",
    "fig, axes = plt.subplots(2, NUM_CLIENTS, figsize=(22, 8), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (client_id, data) in enumerate(random_clients.items()):\n",
    "    # Plot Train\n",
    "    sns.countplot(x=data[\"train\"][\"Class\"], ax=axes[i])\n",
    "    axes[i].set_title(f\"Client {client_id} - Train\")\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "\n",
    "    # Plot Validation\n",
    "    sns.countplot(x=data[\"valid\"][\"Class\"], ax=axes[i + NUM_CLIENTS])\n",
    "    axes[i + NUM_CLIENTS].set_title(f\"Client {client_id} - Val\")\n",
    "    axes[i + NUM_CLIENTS].set_xlabel(\"\")\n",
    "    axes[i + NUM_CLIENTS].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8633300",
   "metadata": {
    "id": "c8633300",
    "outputId": "e147330d-b18a-48aa-b6a6-b43817491b55"
   },
   "outputs": [],
   "source": [
    "non_iid_clients = split_on_non_iid_clients(clients_df, NUM_CLIENTS, VALID_SIZE, alpha=1.0)\n",
    "\n",
    "fig, axes = plt.subplots(2, NUM_CLIENTS, figsize=(22, 8), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (client_id, data) in enumerate(non_iid_clients.items()):\n",
    "    # Plot Train\n",
    "    sns.countplot(x=data[\"train\"][\"Class\"], ax=axes[i])\n",
    "    axes[i].set_title(f\"Client {client_id} - Train\")\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "\n",
    "    # Plot Validation\n",
    "    sns.countplot(x=data[\"valid\"][\"Class\"], ax=axes[i + NUM_CLIENTS])\n",
    "    axes[i + NUM_CLIENTS].set_title(f\"Client {client_id} - Val\")\n",
    "    axes[i + NUM_CLIENTS].set_xlabel(\"\")\n",
    "    axes[i + NUM_CLIENTS].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456a608",
   "metadata": {
    "id": "9456a608"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056609e",
   "metadata": {
    "id": "6056609e"
   },
   "outputs": [],
   "source": [
    "# (ADJUSTMENTS) \n",
    "'''\n",
    "    Altera a configuração da variável `transform`, para:\n",
    "    - Reduzir o overfitting local\n",
    "    - Melhorar a generalização\n",
    "    - Seguir os padrões do ResNet Pré-Treinado\n",
    "'''\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    clients: dict,\n",
    "    test_df: pd.DataFrame,\n",
    "    transforms_config: dict,\n",
    "    batch_size: int = 32,\n",
    "    label_map = dict\n",
    "):\n",
    "    img_size = transforms_config['img_size']\n",
    "    normalize_config = transforms_config['normalizer']\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((224, 224)),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(10),\n",
    "    #     transforms.ColorJitter(\n",
    "    #         brightness=0.2,\n",
    "    #         contrast=0.2,\n",
    "    #         saturation=0.2,\n",
    "    #         hue=0.02\n",
    "    #     ),\n",
    "    #     transforms.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(\n",
    "    #         [0.485, 0.456, 0.406],\n",
    "    #         [0.229, 0.224, 0.225]\n",
    "    #     ),\n",
    "    # ])\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(8),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "        \n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "\n",
    "    for client_id, data in clients.items():\n",
    "        train_dataset = KidneyData(data[\"train\"], label_map, transform)\n",
    "        val_dataset = KidneyData(data[\"valid\"], label_map, transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        trainloaders.append(train_loader)\n",
    "        valloaders.append(val_loader)\n",
    "\n",
    "    test_dataset = KidneyData(test_df, label_map, transform)\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e64e1",
   "metadata": {
    "id": "4c3e64e1"
   },
   "outputs": [],
   "source": [
    "########## \n",
    "random_clients = split_on_clients(clients_df, NUM_CLIENTS, VALID_SIZE)\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(\n",
    "    clients=non_iid_clients,\n",
    "    test_df=test_df,\n",
    "    transforms_config = DATA_TRANSFORM,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    label_map=LABEL2TARGET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0ce55",
   "metadata": {
    "id": "40d0ce55"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cc52c",
   "metadata": {
    "id": "ad7cc52c"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train(model,\n",
    "          trainloader,\n",
    "          valloader,\n",
    "          device,\n",
    "          patience,\n",
    "          global_params=None,\n",
    "          mu=0.01,\n",
    "          verbose=False):\n",
    "\n",
    "    epochs = MODEL_CONFIG['epochs']\n",
    "    criterion = MODEL_CONFIG['criterion']\n",
    "    optimizer_class = MODEL_CONFIG['optimizer']\n",
    "    lr = MODEL_CONFIG['learning_rate']\n",
    "\n",
    "    # Weight decay + only trainable params\n",
    "    optimizer = optimizer_class(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    # Scheduler para estabilizar FL\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=2,\n",
    "        gamma=0.5\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # -----------------------------------\n",
    "            # FEDPROX — TERMO PROXIMAL REAL\n",
    "            # -----------------------------------\n",
    "            if global_params is not None:\n",
    "                prox = 0.0\n",
    "                for w, w_global in zip(model.parameters(), global_params):\n",
    "                    prox += ((w - w_global.to(device)) ** 2).sum()\n",
    "                loss = loss + (mu / 2) * prox\n",
    "            # -----------------------------------\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipping para estabilizar FL\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * images.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "\n",
    "        # Final do epoch\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "        # Step do scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Avaliação no cliente\n",
    "        val_metrics, _ = evaluate(model, valloader, device, num_classes=NUM_CLASSES)\n",
    "        val_loss = val_metrics['loss']\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}: train loss {epoch_loss:.4f}, acc {epoch_acc:.4f}, val_loss {val_loss:.4f}\")\n",
    "\n",
    "    # Retorna o melhor estado\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, testloader, device, num_classes):\n",
    "    model.to(device)\n",
    "    criterion = MODEL_CONFIG['criterion']\n",
    "    model.eval()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # loss ponderado pelo tamanho do batch\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_prob.append(probs.cpu().numpy())\n",
    "\n",
    "    # loss médio real\n",
    "    loss = running_loss / total if total > 0 else 0.0\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.concatenate(y_prob, axis=0)\n",
    "\n",
    "    # AUC CORRIGIDO\n",
    "    try:\n",
    "        if num_classes == 2:\n",
    "            auc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "        else:\n",
    "            auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "\n",
    "    return {\n",
    "        'loss': float(loss),\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision_score(y_true, y_pred, average='weighted', zero_division=0)),\n",
    "        'recall':  float(recall_score(y_true, y_pred, average='weighted', zero_division=0)),\n",
    "        'f1-score': float(f1_score(y_true, y_pred, average='weighted', zero_division=0)),\n",
    "        'auc': auc\n",
    "    }, confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "def get_parameters(model: nn.Module) -> List[np.ndarray]:\n",
    "    return [val.detach().cpu().numpy() for _, val in model.state_dict().items()]\n",
    "\n",
    "def set_parameters(model: nn.Module, parameters: List[np.ndarray]):\n",
    "    state_dict_keys = list(model.state_dict().keys())\n",
    "    new_state = OrderedDict()\n",
    "    for k, arr in zip(state_dict_keys, parameters):\n",
    "        ref = model.state_dict()[k]\n",
    "        t = torch.tensor(arr, dtype=ref.dtype, device=ref.device)\n",
    "        new_state[k] = t\n",
    "    model.load_state_dict(new_state, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612b00c",
   "metadata": {
    "id": "6612b00c"
   },
   "source": [
    "## ResNet18\n",
    "\n",
    "- Here a ResNet18, pretrained with Imagenet dataset will be used, freezing net parameter and fine tuning final layer parameters (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076983d",
   "metadata": {
    "id": "c076983d",
    "outputId": "8be8c501-557b-49cb-e43b-97381f03630a"
   },
   "outputs": [],
   "source": [
    "# original model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cac75",
   "metadata": {
    "id": "9f5cac75"
   },
   "outputs": [],
   "source": [
    "# (ADJUSTMENTS) \n",
    "'''\n",
    "    Pretrained ImageNet melhora MUITO classes difíceis (“Stone” e “Tumor”)\n",
    "    Libera o  `layer4` permitindo o Federated Learning aprender padrões específicos do dataset renal\n",
    "'''\n",
    "\n",
    "# def build_model(num_classes):\n",
    "#     with torch.device(\"cpu\"):\n",
    "#         model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "#         # Congela tudo\n",
    "#         for name, param in model.named_parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         # Descongela só o final da rede\n",
    "#         for name, param in model.layer4.named_parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#         model.fc = nn.Sequential(\n",
    "#             nn.Linear(model.fc.in_features, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, num_classes)\n",
    "#         )\n",
    "\n",
    "#     return model.cpu()\n",
    "\n",
    "# def build_model(num_classes):\n",
    "#     model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "#     # Congela tudo\n",
    "#     for name, param in model.named_parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "#     # Descongela só o final da rede\n",
    "#     for name, param in model.layer4.named_parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "#     model.fc = nn.Sequential(\n",
    "#         nn.Linear(model.fc.in_features, 128),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Dropout(0.2),\n",
    "#         nn.Linear(128, num_classes)\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "# def build_model(num_classes):\n",
    "#     model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#     # Congela tudo exceto classifier\n",
    "#     for name, param in model.named_parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "#     model.classifier[1] = nn.Sequential(\n",
    "#         nn.Dropout(0.3),\n",
    "#         nn.Linear(model.classifier[1].in_features, 256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(256, num_classes)\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "\n",
    "# from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "# def build_model(num_classes):\n",
    "#     model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "#     # congela tudo exceto classifier\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if \"classifier\" not in name:\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     model.classifier = nn.Sequential(\n",
    "#         nn.Linear(model.classifier[0].in_features, 256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Dropout(0.2),\n",
    "#         nn.Linear(256, num_classes)\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# def build_model(num_classes):\n",
    "#     model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "#     # CONGELA TUDO MENOS layer4\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if not ((\"layer4\" in name) or (\"layer3\" in name)):\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     model.fc = nn.Sequential(\n",
    "#         nn.Linear(model.fc.in_features, 256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Dropout(0.3),\n",
    "#         nn.Linear(256, num_classes)\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "def build_model(num_classes):\n",
    "    model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False   # <-- congela TUDO\n",
    "\n",
    "    # só treina a última camada\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2946b9",
   "metadata": {
    "id": "ee2946b9"
   },
   "source": [
    "- Adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947c09d",
   "metadata": {
    "id": "d947c09d",
    "outputId": "17b69445-78c8-43f3-cf88-23253ef1b41c"
   },
   "outputs": [],
   "source": [
    "model = build_model(num_classes = NUM_CLASSES)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44935661",
   "metadata": {
    "id": "44935661",
    "outputId": "f355b9c8-4a1c-4120-b4c3-d733a4a0fc38"
   },
   "outputs": [],
   "source": [
    "# Check trainable and not trainable layers\n",
    "trainable_layers = []\n",
    "frozen_layers = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_layers.append(name)\n",
    "    else:\n",
    "        frozen_layers.append(name)\n",
    "\n",
    "print(\"Trainable layers:\")\n",
    "for name in trainable_layers:\n",
    "    print(name)\n",
    "\n",
    "print(\"\\nFrozen layers:\")\n",
    "for name in frozen_layers:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d54793",
   "metadata": {
    "id": "c7d54793"
   },
   "source": [
    "# Flower and Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373163a5",
   "metadata": {
    "id": "373163a5"
   },
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e4a55",
   "metadata": {
    "id": "017e4a55"
   },
   "outputs": [],
   "source": [
    "# (ADJUSTMENTS)\n",
    "BASE_MODEL = build_model(NUM_CLASSES)\n",
    "BASE_MODEL = BASE_MODEL.to(\"cpu\")             # <-- IMPORTANTE\n",
    "BASE_STATE = {k: v.cpu() for k, v in BASE_MODEL.state_dict().items()}\n",
    "\n",
    "class KidneyClient(fl.client.NumPyClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        trainloader,\n",
    "        valloader,\n",
    "        device,\n",
    "        num_classes\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # RETORNA OS PARÂMETROS LOCAIS\n",
    "    # --------------------------------------------------------\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # RECEBE OS PESOS DO MODELO GLOBAL (VENDA DO SERVIDOR)\n",
    "    # --------------------------------------------------------\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # FIT — AGORA COM FEDPROX\n",
    "    # --------------------------------------------------------\n",
    "    def fit(self, parameters, config):\n",
    "        # 1. RECEBE PESOS GLOBAIS\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # 2. SALVA PESOS GLOBAIS PARA O FEDPROX\n",
    "        global_params = [p.clone().detach() for p in self.model.parameters()]\n",
    "\n",
    "        # 3. TREINAMENTO LOCAL\n",
    "        trained_model = train(\n",
    "            model=self.model,\n",
    "            trainloader=self.trainloader,\n",
    "            valloader=self.valloader,\n",
    "            device=self.device,\n",
    "            patience=2,\n",
    "            global_params=global_params,   # <--\n",
    "            mu=0.1,               # <--\n",
    "            verbose=False\n",
    "        )\n",
    "        # 4. RETORNA OS PESOS LOCAIS APÓS O TREINO\n",
    "        return (\n",
    "            [val.cpu().numpy() for _, val in trained_model.state_dict().items()],\n",
    "            len(self.trainloader.dataset),\n",
    "            {}\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # EVALUATE (sem mudanças)\n",
    "    # --------------------------------------------------------\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        metrics, _ = evaluate(self.model, self.valloader, self.device, num_classes=self.num_classes)\n",
    "        return float(metrics[\"loss\"]), len(self.valloader.dataset), metrics\n",
    "\n",
    "    # def evaluate(self, parameters, config):\n",
    "    #     self.set_parameters(parameters)\n",
    "    #     self.model.to(self.device)\n",
    "\n",
    "    #     # Avaliação no TRAIN e não no VAL\n",
    "    #     metrics, _ = evaluate(\n",
    "    #         self.model, \n",
    "    #         self.trainloader, \n",
    "    #         self.device,\n",
    "    #         num_classes=self.num_classes\n",
    "    #     )\n",
    "\n",
    "    #     return float(metrics[\"loss\"]), len(self.trainloader.dataset), metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(cid: str):\n",
    "    \"\"\"Create a Kidney client representing a single organization.\"\"\"\n",
    "\n",
    "    model = build_model(num_classes=NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "\n",
    "    # return KidneyClient(model, trainloader, valloader, DEVICE, NUM_CLASSES).to_client()\n",
    "    return KidneyClient(model, trainloader, valloader, DEVICE, NUM_CLASSES)\n",
    "\n",
    "\n",
    "# def client_fn(cid: str):\n",
    "\n",
    "#     # 1) Cria modelo SEM MOVER PARA GPU\n",
    "#     model = build_model(num_classes=NUM_CLASSES)\n",
    "    \n",
    "#     # 2) NÃO FAÇA model.to(DEVICE) AQUI\n",
    "#     # Flower vai pegar os pesos no CPU\n",
    "\n",
    "#     trainloader = trainloaders[int(cid)]\n",
    "#     valloader = valloaders[int(cid)]\n",
    "\n",
    "#     return KidneyClient(\n",
    "#         model=model,\n",
    "#         trainloader=trainloader,\n",
    "#         valloader=valloader,\n",
    "#         device=DEVICE,          # a GPU só será usada dentro do fit()\n",
    "#         num_classes=NUM_CLASSES\n",
    "#     )\n",
    "\n",
    "\n",
    "# def client_fn(cid: str):\n",
    "#     model = build_model(NUM_CLASSES)\n",
    "\n",
    "#     # Carrega PESOS DO CPU SEMPRE\n",
    "#     model.load_state_dict(BASE_STATE)\n",
    "\n",
    "#     trainloader = trainloaders[int(cid)]\n",
    "#     valloader = valloaders[int(cid)]\n",
    "\n",
    "#     return KidneyClient(\n",
    "#         model=model,\n",
    "#         trainloader=trainloader,\n",
    "#         valloader=valloader,\n",
    "#         device=DEVICE,\n",
    "#         num_classes=NUM_CLASSES\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad003551",
   "metadata": {
    "id": "ad003551"
   },
   "source": [
    "## Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e738f",
   "metadata": {
    "id": "165e738f"
   },
   "outputs": [],
   "source": [
    "def server_evaluate_fn(save_path: str = '.'):\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        model = build_model(NUM_CLASSES)\n",
    "\n",
    "        set_parameters(model, parameters)\n",
    "        metrics, cm = evaluate(model, testloader, DEVICE, NUM_CLASSES)\n",
    "        loss = float(metrics['loss'])\n",
    "\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        if server_round != 0:\n",
    "            torch.save(model.state_dict(), f\"{save_path}/model_round{server_round}.pt\")\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    return evaluate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8833c3f",
   "metadata": {
    "id": "c8833c3f"
   },
   "source": [
    "### Aggregation strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d84156",
   "metadata": {
    "id": "90d84156"
   },
   "source": [
    "- FedAvg, FedAvg, FedAdagrad, FedAdam, FedYogi, Krum, DPFedAvgAdaptive, QFedAvg, FaultTolerantFedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65091d",
   "metadata": {
    "id": "2d65091d"
   },
   "outputs": [],
   "source": [
    "# (ADJUSTMENTS)\n",
    "def metrics_agg(metrics):\n",
    "    # metrics = [(num_examples, {\"loss\": ..., \"accuracy\": ...}), ...]\n",
    "    total_examples = sum(num for num, _ in metrics)\n",
    "    aggregated = {}\n",
    "    for key in metrics[0][1].keys():\n",
    "        aggregated[key] = sum(num * m[key] for num, m in metrics) / total_examples\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# CONFIGURAÇÕES ENVIADAS AOS CLIENTES (FedProx precisa disso)\n",
    "# -----------------------------------------------------------------\n",
    "def fit_config(server_round: int):\n",
    "    return {\n",
    "        \"round\": server_round,\n",
    "        \"local_epochs\": MODEL_CONFIG[\"epochs\"],\n",
    "    }\n",
    "\n",
    "def eval_config(server_round: int):\n",
    "    return {\n",
    "        \"round\": server_round\n",
    "    }\n",
    "\n",
    "\n",
    "from flwr.server.strategy import FedProx, FedAvg, FedAdagrad, FedAdam, FedYogi, Krum, DPFedAvgAdaptive, QFedAvg, FaultTolerantFedAvg\n",
    "import flwr as fl\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# ESTRATÉGIA FEDPROX FINAL (COMPLETA + CORRIGIDA)\n",
    "# -----------------------------------------------------------------\n",
    "# strategy = FedProx(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     min_fit_clients=NUM_CLIENTS,\n",
    "#     min_evaluate_clients=NUM_CLIENTS,\n",
    "#     min_available_clients=NUM_CLIENTS,\n",
    "#     proximal_mu=0.05,\n",
    "#     evaluate_fn=server_evaluate_fn(save_path='models_25_rounds'),\n",
    "#     evaluate_metrics_aggregation_fn=metrics_agg\n",
    "# )\n",
    "\n",
    "# strategy = FedAvg(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     min_fit_clients=NUM_CLIENTS,\n",
    "#     min_evaluate_clients=NUM_CLIENTS,\n",
    "#     min_available_clients=NUM_CLIENTS,\n",
    "#     evaluate_fn=server_evaluate_fn(save_path='models_25_rounds'),\n",
    "#     evaluate_metrics_aggregation_fn=metrics_agg,\n",
    "# )\n",
    "\n",
    "strategy = FedAdagrad(\n",
    "    fraction_fit=1.0,\n",
    "    fraction_evaluate=1.0,\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    eta=0.1,\n",
    "    tau=1e-9,\n",
    "    evaluate_fn=server_evaluate_fn(save_path='models_22_rounds_fedadagrad'),\n",
    "    evaluate_metrics_aggregation_fn=metrics_agg\n",
    ")\n",
    "\n",
    "# strategy = FedAdam(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     min_fit_clients=NUM_CLIENTS,\n",
    "#     min_evaluate_clients=NUM_CLIENTS,\n",
    "#     min_available_clients=NUM_CLIENTS,\n",
    "#     eta=0.001,         # taxa de aprendizado global\n",
    "#     eta_l=0.001,       # taxa de aprendizado local\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.99,\n",
    "#     tau=1e-9,\n",
    "#     evaluate_fn=server_evaluate_fn(save_path='models_22_rounds_fedadam'),\n",
    "#     evaluate_metrics_aggregation_fn=metrics_agg,\n",
    "# )\n",
    "\n",
    "# strategy = FedYogi(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     min_fit_clients=NUM_CLIENTS,\n",
    "#     min_evaluate_clients=NUM_CLIENTS,\n",
    "#     min_available_clients=NUM_CLIENTS,\n",
    "#     eta=0.001,         # taxa de aprendizado global\n",
    "#     eta_l=0.001,       # taxa de aprendizado local\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.99,\n",
    "#     tau=1e-9,\n",
    "#     evaluate_fn=server_evaluate_fn(save_path='models_22_rounds_fedyogi'),\n",
    "#     evaluate_metrics_aggregation_fn=metrics_agg,\n",
    "# )\n",
    "\n",
    "# strategy = Krum(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     min_fit_clients=NUM_CLIENTS,\n",
    "#     min_evaluate_clients=NUM_CLIENTS,\n",
    "#     min_available_clients=NUM_CLIENTS,\n",
    "#     krum_config={\"multi_krum\": False, \"num_krum\": 1},  # ajustar conforme necessário\n",
    "#     evaluate_fn=server_evaluate_fn(save_path='models_22_rounds_krum'),\n",
    "#     evaluate_metrics_aggregation_fn=metrics_agg,\n",
    "# )\n",
    "\n",
    "# strategy = DPFedAvgAdaptive(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     min_fit_clients=NUM_CLIENTS,\n",
    "#     min_evaluate_clients=NUM_CLIENTS,\n",
    "#     min_available_clients=NUM_CLIENTS,\n",
    "#     noise_multiplier=1.0,           # ajustar conforme necessário\n",
    "#     l2_norm_clip=1.0,               # ajustar conforme necessário\n",
    "#     adaptive_clip=True,             # ativa o clipping adaptativo\n",
    "#     evaluate_fn=server_evaluate_fn(save_path='models_22_rounds_dp_fedavg_adaptive'),\n",
    "#     evaluate_metrics_aggregation_fn=metrics_agg,\n",
    "# )\n",
    "\n",
    "# strategy = QFedAvg(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     min_fit_clients=NUM_CLIENTS,\n",
    "#     min_evaluate_clients=NUM_CLIENTS,\n",
    "#     min_available_clients=NUM_CLIENTS,\n",
    "#     q_param=0.5,  # ajustar conforme desejado\n",
    "#     evaluate_fn=server_evaluate_fn(save_path='models_22_rounds_qfedavg'),\n",
    "#     evaluate_metrics_aggregation_fn=metrics_agg,\n",
    "# )\n",
    "\n",
    "# strategy = FaultTolerantFedAvg(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     min_fit_clients=NUM_CLIENTS,\n",
    "#     min_evaluate_clients=NUM_CLIENTS,\n",
    "#     min_available_clients=NUM_CLIENTS,\n",
    "#     evaluate_fn=server_evaluate_fn(save_path='models_22_rounds_fault_toler_fedavg'),\n",
    "#     evaluate_metrics_aggregation_fn=metrics_agg,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e9a20",
   "metadata": {
    "id": "fe0e9a20"
   },
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18336762",
   "metadata": {
    "id": "18336762",
    "outputId": "29bf32ab-7e9c-4000-e42d-1a627752a4b7"
   },
   "outputs": [],
   "source": [
    "print(f\"running in {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70203b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7f810",
   "metadata": {
    "id": "24d7f810",
    "outputId": "d8e34394-36cc-423b-ebbf-2c26f25374a0"
   },
   "outputs": [],
   "source": [
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=ROUNDS),\n",
    "    strategy=strategy,\n",
    "    client_resources={\n",
    "        \"num_cpus\": 1,\n",
    "        \"num_gpus\": 0.25\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb42278",
   "metadata": {
    "id": "ddb42278"
   },
   "source": [
    "### Show Federated learning history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae388e9f",
   "metadata": {
    "id": "ae388e9f",
    "outputId": "615e23fc-9456-43c7-f0fa-db179f29486d"
   },
   "outputs": [],
   "source": [
    "print(\"All history: \", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07050646",
   "metadata": {
    "id": "07050646",
    "outputId": "a8e52b02-55f2-4e8d-f9a9-550b0c81488b"
   },
   "outputs": [],
   "source": [
    "print(\"Output FL - Loss: \", history.losses_distributed)\n",
    "print(\"Output FL - Accuracy: \", history.metrics_distributed)\n",
    "\n",
    "print(\"Centralized metrics:\", history.metrics_centralized)\n",
    "print(\"Centralized losses:\", history.losses_centralized)\n",
    "\n",
    "\n",
    "fl_loss = []\n",
    "fl_accuracy = []\n",
    "\n",
    "for loss in history.losses_distributed:\n",
    "  print(f\"Loss in Round {loss[0]}: {loss[1]}\")\n",
    "  fl_loss.append(loss[1])\n",
    "\n",
    "for accuracy in history.metrics_distributed[\"accuracy\"]:\n",
    "  print(f\"Accuracy in Round {accuracy[0]}: {accuracy[1]}\")\n",
    "  fl_accuracy.append(accuracy[1])\n",
    "\n",
    "communication_round = range(len(fl_loss))\n",
    "my_dpi=96\n",
    "plt.figure(figsize=(800/my_dpi, 600/my_dpi), dpi=my_dpi)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.plot(communication_round, fl_loss, linewidth=1, linestyle=\"solid\", marker=\".\", color=\"black\")\n",
    "plt.xlabel(\"Communication Round(#)\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=18)\n",
    "plt.grid(linestyle=':', linewidth='0.5')\n",
    "\n",
    "############################################\n",
    "communication_round = range(len(fl_accuracy))\n",
    "my_dpi=96\n",
    "plt.figure(figsize=(800/my_dpi, 600/my_dpi), dpi=my_dpi)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.plot(communication_round, fl_accuracy, linewidth=1, linestyle=\"solid\", marker=\".\", color=\"black\")\n",
    "plt.xlabel(\"Communication Round(#)\", fontsize=18)\n",
    "plt.ylabel(\"Accuracy\", fontsize=18)\n",
    "plt.grid(linestyle=':', linewidth='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c9e7f",
   "metadata": {
    "id": "978c9e7f"
   },
   "source": [
    "# Server Test set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77263660",
   "metadata": {
    "id": "77263660",
    "outputId": "51109da4-08c0-413f-ad87-69c69ae3ae08"
   },
   "outputs": [],
   "source": [
    "# (ADJUSTMENTS)\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# IDENTIFICAR MODELO GLOBAL (MELHOR ROUND)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "model_dir = \"models_25_rounds\"\n",
    "\n",
    "# lista arquivos tipo model_roundX.pt\n",
    "round_files = [f for f in os.listdir(model_dir) if f.startswith(\"model_round\")]\n",
    "\n",
    "if len(round_files) == 0:\n",
    "    raise RuntimeError(f\"Nenhum modelo encontrado em {model_dir}\")\n",
    "\n",
    "# extrai números dos rounds\n",
    "round_numbers = sorted([\n",
    "    int(f.replace(\"model_round\", \"\").replace(\".pt\", \"\"))\n",
    "    for f in round_files\n",
    "])\n",
    "\n",
    "# tenta usar o histórico do Flower para escolher melhor modelo\n",
    "try:\n",
    "    # history.losses_distributed = [(round, loss), ...]\n",
    "    losses = history.losses_distributed\n",
    "    best_round = min(losses, key=lambda x: x[1])[0]\n",
    "except:\n",
    "    # fallback: usa último round\n",
    "    best_round = round_numbers[-1]\n",
    "\n",
    "best_model_path = os.path.join(model_dir, f\"model_round{best_round}.pt\")\n",
    "print(f\"\\n🔍 Carregando melhor modelo encontrado: Round {best_round}\")\n",
    "print(f\"📁 Path: {best_model_path}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CARREGAR O MODELO GLOBAL\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "global_model = build_model(num_classes=NUM_CLASSES)\n",
    "global_model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "global_model.to(DEVICE)\n",
    "global_model.eval()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# AVALIAÇÃO GLOBAL\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "metrics, cm = evaluate(global_model, testloader, DEVICE, NUM_CLASSES)\n",
    "\n",
    "print(\"\\n📊 Métricas do Melhor Modelo Global:\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01fc0ef",
   "metadata": {
    "id": "f01fc0ef",
    "outputId": "0aef7fab-c773-48d6-af81-d6bee98ee764"
   },
   "outputs": [],
   "source": [
    "# (ADJUSTMENTS)\n",
    "# ----------------------------\n",
    "# CONFUSION MATRIX (BEST ROUND)\n",
    "# ----------------------------\n",
    "\n",
    "labels_map = {0: 'Cyst', 1: 'Normal', 2: 'Stone', 3: 'Tumor'}\n",
    "labels = [labels_map[i] for i in range(len(labels_map))]\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels\n",
    ")\n",
    "plt.title(f\"Confusion Matrix - Best Round {best_round}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "\n",
    "# opcional: salva figura para relatório\n",
    "plt.savefig(f\"confusion_matrix_round_{best_round}.png\", dpi=200, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98414d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# AVALIAÇÃO CENTRALIZADA FINAL\n",
    "# ----------------------------\n",
    "\n",
    "print(\"\\n🔎 Avaliando o melhor modelo global no conjunto de TESTE...\\n\")\n",
    "\n",
    "metrics, cm = evaluate(global_model, testloader, DEVICE, NUM_CLASSES)\n",
    "\n",
    "print(\"📊 Resultados finais (Centralized Evaluation):\")\n",
    "print(f\" - Loss:      {metrics['loss']:.6f}\")\n",
    "print(f\" - Accuracy:  {metrics['accuracy']:.6f}\")\n",
    "print(f\" - Precision: {metrics['precision']:.6f}\")\n",
    "print(f\" - Recall:    {metrics['recall']:.6f}\")\n",
    "print(f\" - F1-score:  {metrics['f1-score']:.6f}\")\n",
    "print(f\" - AUC:       {metrics['auc']:.6f}\")\n",
    "\n",
    "# salva para logs\n",
    "centralized_metrics = metrics\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
